{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'sparkts'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-25e233082b1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStructType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStructField\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTimestampType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoubleType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStringType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msparkts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetimeindex\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0muniform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBusinessDayFrequency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msparkts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeseriesrdd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime_series_rdd_from_observations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'sparkts'"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from pyspark import SparkContext, SQLContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, TimestampType, DoubleType, StringType\n",
    "\n",
    "from sparkts.datetimeindex import uniform, BusinessDayFrequency\n",
    "from sparkts.timeseriesrdd import time_series_rdd_from_observations\n",
    "\n",
    "def lineToRow(line):\n",
    "    (year, month, day, symbol, volume, price) = line.split(\"\\t\")\n",
    "    # Python 2.x compatible timestamp generation\n",
    "    dt = datetime(int(year), int(month), int(day))\n",
    "    return (dt, symbol, float(price))\n",
    "\n",
    "def loadObservations(sparkContext, sqlContext, path):\n",
    "    textFile = sparkContext.textFile(path)\n",
    "    rowRdd = textFile.map(lineToRow)\n",
    "    schema = StructType([\n",
    "        StructField('timestamp', TimestampType(), nullable=True),\n",
    "        StructField('symbol', StringType(), nullable=True),\n",
    "        StructField('price', DoubleType(), nullable=True),\n",
    "    ])\n",
    "    return sqlContext.createDataFrame(rowRdd, schema);\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sc = SparkContext(appName=\"Stocks\")\n",
    "    sqlContext = SQLContext(sc)\n",
    "\n",
    "    tickerObs = loadObservations(sc, sqlContext, \"../data/ticker.tsv\")\n",
    "    \n",
    "    # Create an daily DateTimeIndex over August and September 2015\n",
    "    freq = BusinessDayFrequency(1, 1, sc)\n",
    "    dtIndex = uniform(start='2015-08-03T00:00-07:00', end='2015-09-22T00:00-07:00', freq=freq, sc=sc)\n",
    "    \n",
    "    # Align the ticker data on the DateTimeIndex to create a TimeSeriesRDD\n",
    "    tickerTsrdd = time_series_rdd_from_observations(dtIndex, tickerObs, \"timestamp\", \"symbol\", \"price\")\n",
    "\n",
    "    # Cache it in memory\n",
    "    tickerTsrdd.cache()\n",
    "    \n",
    "    # Count the number of series (number of symbols)\n",
    "    print(tickerTsrdd.count())\n",
    "    \n",
    "    # Impute missing values using linear interpolation\n",
    "    filled = tickerTsrdd.fill(\"linear\")\n",
    "    \n",
    "    # Compute return rates\n",
    "    returnRates = filled.return_rates()\n",
    "    \n",
    "    # Durbin-Watson test for serial correlation, ported from TimeSeriesStatisticalTests.scala\n",
    "    def dwtest(residuals):\n",
    "        residsSum = residuals[0] * residuals[0]\n",
    "        diffsSum = 0.0\n",
    "        i = 1\n",
    "        while i < len(residuals):\n",
    "            residsSum += residuals[i] * residuals[i]\n",
    "            diff = residuals[i] - residuals[i - 1]\n",
    "            diffsSum += diff * diff\n",
    "            i += 1\n",
    "        return diffsSum / residsSum\n",
    "    \n",
    "    # Compute Durbin-Watson stats for each series\n",
    "    # Swap ticker symbol and stats so min and max compare the statistic value, not the\n",
    "    # ticker names.\n",
    "    dwStats = returnRates.map_series(lambda row: (row[0], [dwtest(row[1])])).map(lambda x: (x[1], x[0]))\n",
    "    \n",
    "    print(dwStats.min())\n",
    "    print(dwStats.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
